{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with Redshift and the Feature Store\n",
    "\n",
    "This tutorial notebook will help you get started with working with the Hopsworks feature store and Redshift.\n",
    "\n",
    "\n",
    "The tutorial is divided in 2 parts: \n",
    "* [Create a sample Amazon Redshift cluster](#setup_redhsift)\n",
    "* [Load sample data into Redshift cluster](#load_data_redhsift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sample Amazon Redshift cluster<a name=\"setup_redhsift\"></a>\n",
    "\n",
    "##### This step will describe how to  Create a sample Amazon Redshift cluster. If you already have Redshift cluster you may skip this step \n",
    "\n",
    "To setup sample redshift cluster please follow the steps [here](https://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-launch-sample-cluster.html)\n",
    "\n",
    "To to intrecat with redshift cluser from hopsworks modify your redshift cluster's follow steps bellow:\n",
    "\n",
    "\n",
    "1) Make sure that your EC2 instance, that hopsworks runs on, has AIM role with Redshift access policies \n",
    "2) Make sure that your redshift cluster's inbound traffic rules allowe access from hopsworks EC2 instance:\n",
    "     * From AWS management console go to VPC\n",
    "     * From VPC go to Security Groups\n",
    "     * Create new secuiry group or if you have already created modify Inbound rules:\n",
    "         a) go to Inbound rules \n",
    "         b) Edit inbound rules\n",
    "         c) Add rule\n",
    "         e) In Type section select Redshift\n",
    "         f) In Source section select Custom\n",
    "         g) Then add name of security group of your hopsworks EC2 instance \n",
    "         \n",
    "3) [Download an Amazon Redshift JDBC driver](https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html#download-jdbc-driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  sample data into Redshift cluster <a name=\"load_data_redhsift\"></a>\n",
    "\n",
    "### Import a CSV in Redshift from s3 bucket\n",
    "\n",
    "Importing a CSV into Redshift requires you to create a table first. \n",
    "\n",
    "<code>\n",
    "    CREATE TABLE telcom (\n",
    "        customer_id VARCHAR primary key \n",
    "        gender VARCHAR,   \n",
    "        senior_citizen VARCHAR, \n",
    "        partner VARCHAR,\n",
    "        dependents VARCHAR,  \n",
    "        tenure INTEGER, \n",
    "        phone_service VARCHAR,      \n",
    "        multiple_lines VARCHAR, \n",
    "        internet_service VARCHAR, \n",
    "        online_security VARCHAR, \n",
    "        online_backup VARCHAR, \n",
    "        device_protection VARCHAR, \n",
    "        tech_support VARCHAR, \n",
    "        streaming_tv VARCHAR,\n",
    "        streaming_movies VARCHAR,        \n",
    "        contract VARCHAR,\n",
    "        paperless_billing VARCHAR,            \n",
    "        payment_method INTEGER, \n",
    "        monthly_charges INTEGER, \n",
    "        total_charges INTEGER, \n",
    "        churn VARCHAR    \n",
    "    );\n",
    "</code>\n",
    "\n",
    "and then copy\n",
    "\n",
    "<code>\n",
    "    COPY telcom\n",
    "        FROM 's3://<your-bucket-name>/load/file_name.csv'\n",
    "        credentials 'aws_access_key_id=<Your-Access-Key-ID>;aws_secret_access_key=<Your-Secret-Access-Key>'\n",
    "    CSV;\n",
    "</code>\n",
    "    \n",
    "please refer to the [Redshift COPY Command Specification](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html) for a complete list of options for COPY,     \n",
    "\n",
    "    \n",
    "### Import a telcom data in Redshift from hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.api.java.JavaSparkContext;\n",
    "import org.apache.spark.sql.DataFrameWriter;\n",
    "import org.apache.spark.sql.Dataset;\n",
    "import org.apache.spark.sql.Row;\n",
    "import org.apache.spark.sql.SaveMode;\n",
    "import org.apache.spark.sql.SparkSession;\n",
    "import io.hops.util.Hops\n",
    "import org.apache.spark.sql._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jdbcUsername = \"YOUR_REDSHIFT_USER_NAME\"\n",
    "val AIMrole = \"AIM_name_of_EC2_with_redshift_access\"\n",
    "val jdbcHostname = \"redshift-cluster-1.citpxgaovgkr.eu-north-1.redshift.amazonaws.com\"\n",
    "val jdbcPort = 5439\n",
    "val jdbcDatabase = \"telcom\"\n",
    "//val jdbcUrl = s\"jdbc:redshift://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}\"\n",
    "val jdbcUrl = s\"jdbc:redshift:iam://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom = spark.read.csv(\"path\")\n",
    "\n",
    "telcom\n",
    "  write.\n",
    "  format(\"jdbc\").\n",
    "  option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\").\n",
    "  option(\"url\",jdbcUrl).\n",
    "  option(\"dbtable\", jdbcDatabase).\n",
    "  option(\"user\", jdbcUsername).\n",
    "  option(\"aws_iam_role\", AIMrole).\n",
    "  mode(\"append\"). \n",
    "  save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
